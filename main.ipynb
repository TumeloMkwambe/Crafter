{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7312626-916f-4327-a937-a4df977808fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluation_episode, image_display\n",
    "import stable_baselines3 as sb3\n",
    "from env_setup import init_env\n",
    "import actor_critic\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc7c201d-707c-4f38-8d71-422543d7e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class Actor_Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = models.resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        self.actor = nn.Linear(resnet.fc.in_features, n_actions)\n",
    "\n",
    "        self.critic = nn.Linear(resnet.fc.in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        logits = self.actor(features)\n",
    "        \n",
    "        values = self.critic(features)\n",
    "\n",
    "        return logits, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ab244b37-7118-46d1-bb67-6ec87f4f20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "\n",
    "    def __init__(self, vec_env, model, n_steps, gamma, lambda_):\n",
    "        \n",
    "        self.vec_env = vec_env\n",
    "        self.model = model\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        self.data = {\"observations\": [], \"actions\": [], \"log_probs\": [], \"values\": [], \"rewards\": [], \"dones\": [], \"advantages\": [], \"infos\": []}\n",
    "        self.count_steps = 0\n",
    "\n",
    "    def observation_preprocessing(self, observations):\n",
    "        \n",
    "        observations = torch.tensor(observations)\n",
    "        observations = observations.permute(0, 3, 1, 2) # from [N, H, W, C] to [N, C, H, W]\n",
    "        observations = observations.float() / 255.0\n",
    "        \n",
    "        return observations\n",
    "\n",
    "    def data_collection(self):\n",
    "        \n",
    "        observations = self.vec_env.reset()\n",
    "        \n",
    "        while self.count_steps < self.n_steps:\n",
    "\n",
    "            observations = self.observation_preprocessing(observations)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                logits, values = self.model(observations)\n",
    "                \n",
    "                distros = torch.distributions.Categorical(logits = logits)\n",
    "                \n",
    "                actions = distros.sample().unsqueeze(-1)\n",
    "                \n",
    "                log_probs = distros.log_prob(actions.squeeze(-1)).unsqueeze(-1)\n",
    "                \n",
    "            next_observations, rewards, dones, infos = self.vec_env.step(actions.cpu().numpy().squeeze(-1))\n",
    "            \n",
    "            self.data[\"observations\"].append(observations) \n",
    "            self.data[\"actions\"].append(actions)\n",
    "            self.data[\"log_probs\"].append(log_probs)\n",
    "            self.data[\"values\"].append(values)\n",
    "            self.data[\"rewards\"].append(rewards)\n",
    "            self.data[\"dones\"].append(dones)\n",
    "            self.data[\"infos\"].append(infos)\n",
    "            \n",
    "            self.count_steps += self.vec_env.num_envs\n",
    "            observations = next_observations\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            observations = self.observation_preprocessing(observations)\n",
    "\n",
    "            _, values = self.model(observations)\n",
    "\n",
    "            self.data[\"values\"].append(values)\n",
    "\n",
    "\n",
    "    def stack(self):\n",
    "\n",
    "        obs = self.data[\"observations\"][0].shape\n",
    "\n",
    "        self.data[\"observations\"] = torch.stack(self.data[\"observations\"])\n",
    "        \n",
    "        self.data[\"actions\"] = torch.cat(self.data[\"actions\"]).view(len(self.data[\"actions\"]), -1)\n",
    "\n",
    "        self.data[\"log_probs\"] = torch.cat(self.data[\"log_probs\"]).view(len(self.data[\"log_probs\"]), -1)\n",
    "\n",
    "        self.data[\"values\"] = torch.cat(self.data[\"values\"]).view(len(self.data[\"values\"]), -1)\n",
    "\n",
    "        self.data[\"rewards\"] = torch.tensor(np.stack(self.data[\"rewards\"]), dtype = torch.float32)\n",
    "        \n",
    "        self.data[\"dones\"] = torch.tensor(np.stack(self.data[\"dones\"]), dtype = torch.float32)\n",
    "    \n",
    "    def advantages_collector(self):\n",
    "\n",
    "        rewards = self.data[\"rewards\"]\n",
    "\n",
    "        values = self.data[\"values\"]\n",
    "\n",
    "        dones = self.data[\"dones\"]\n",
    "\n",
    "        T, n_envs = rewards.shape\n",
    "        \n",
    "        gae = torch.zeros(n_envs)\n",
    "        \n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            gae = delta + self.gamma * self.lambda_ * (1 - dones[t]) * gae\n",
    "            \n",
    "            advantages[t] = gae\n",
    "        \n",
    "        self.data[\"advantages\"] = advantages\n",
    "        \n",
    "        self.data[\"returns\"] = advantages + values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e35636d0-59b1-47d9-a44a-9509b395c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vec = sb3.common.env_util.make_vec_env(env_id = init_env, n_envs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b25b752a-3b7b-4fba-9854-d44083f18830",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = Actor_Critic(n_actions = 4)\n",
    "ppo = PPO(vec_env = env_vec, model = ac, n_steps = 15, gamma = 1, lambda_ = 3)\n",
    "ppo.data_collection()\n",
    "ppo.stack()\n",
    "ppo.advantages_collector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b3c3ed52-a691-4456-92a1-5b22aa49a9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 3, 64, 64])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(ppo.data[\"observations\"].shape)  # [T, num_envs, C, H, W]\n",
    "print(ppo.data[\"actions\"].shape)       # [T, num_envs]\n",
    "print(ppo.data[\"advantages\"].shape)    # [T, num_envs]\n",
    "print(ppo.data[\"returns\"].shape)       # [T, num_envs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ee5d8-01a2-4b0e-8cd2-ab1a5996246e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3379f-06f2-4b26-816d-065058449023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de55627-1c90-4e3b-8757-ebf6a54c86df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
