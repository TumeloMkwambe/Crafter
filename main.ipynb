{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7312626-916f-4327-a937-a4df977808fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluation_episode, image_display\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import stable_baselines3 as sb3\n",
    "from env_setup import init_env\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, observations, actions, log_probs, values, returns, advantages):\n",
    "        \n",
    "        self.observations = observations\n",
    "        \n",
    "        self.actions = actions\n",
    "        \n",
    "        self.log_probs = log_probs\n",
    "        \n",
    "        self.values = values\n",
    "        \n",
    "        self.returns = returns\n",
    "        \n",
    "        self.advantages = advantages\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        obs = self.observations[idx]\n",
    "        \n",
    "        acts = self.actions[idx]\n",
    "        \n",
    "        log_probs = self.actions[idx]\n",
    "        \n",
    "        values = self.values[idx]\n",
    "        \n",
    "        returns = self.returns[idx]\n",
    "        \n",
    "        advantages = self.advantages[idx]\n",
    "\n",
    "        return obs, acts, log_probs, values, returns, advantages\n",
    "\n",
    "class PPO:\n",
    "\n",
    "    def __init__(self, vec_env, model, config):\n",
    "        \n",
    "        self.vec_env = vec_env\n",
    "        self.model = model\n",
    "        self.n_steps = config['N_STEPS']\n",
    "        self.gamma = config['GAMMA']\n",
    "        self.lambda_ = config['LAMBDA']\n",
    "\n",
    "        self.data = {\"observations\": [], \"actions\": [], \"log_probs\": [], \"values\": [], \"rewards\": [], \"dones\": [], \"infos\": []}\n",
    "        self.count_steps = 0\n",
    "\n",
    "    def observation_preprocessing(self, observations):\n",
    "        \n",
    "        observations = torch.tensor(observations)\n",
    "        observations = observations.permute(0, 3, 1, 2) # from [N, H, W, C] to [N, C, H, W]\n",
    "        observations = observations.float() / 255.0\n",
    "        \n",
    "        return observations\n",
    "\n",
    "    def data_collection(self):\n",
    "        \n",
    "        observations = self.vec_env.reset()\n",
    "        \n",
    "        while self.count_steps < self.n_steps:\n",
    "\n",
    "            observations = self.observation_preprocessing(observations)\n",
    "\n",
    "            self.model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                print(f'observations: {observations.shape} | {observations.dtype}')\n",
    "                \n",
    "                logits, values = self.model(observations)\n",
    "                \n",
    "                distros = torch.distributions.Categorical(logits = logits)\n",
    "                \n",
    "                actions = distros.sample().unsqueeze(-1)\n",
    "                \n",
    "                log_probs = distros.log_prob(actions.squeeze(-1)).unsqueeze(-1)\n",
    "                \n",
    "            next_observations, rewards, dones, infos = self.vec_env.step(actions.cpu().numpy().squeeze(-1))\n",
    "            \n",
    "            self.data[\"observations\"].append(observations) \n",
    "            self.data[\"actions\"].append(actions)\n",
    "            self.data[\"log_probs\"].append(log_probs)\n",
    "            self.data[\"values\"].append(values)\n",
    "            self.data[\"rewards\"].append(rewards)\n",
    "            self.data[\"dones\"].append(dones)\n",
    "            self.data[\"infos\"].append(infos)\n",
    "            \n",
    "            self.count_steps += self.vec_env.num_envs\n",
    "            observations = next_observations\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            observations = self.observation_preprocessing(observations)\n",
    "\n",
    "            _, values = self.model(observations)\n",
    "\n",
    "            self.data[\"values\"].append(values)\n",
    "\n",
    "\n",
    "    def stack(self):\n",
    "\n",
    "        obs = self.data[\"observations\"][0].shape\n",
    "\n",
    "        self.data[\"observations\"] = torch.stack(self.data[\"observations\"])\n",
    "        \n",
    "        self.data[\"actions\"] = torch.cat(self.data[\"actions\"]).view(len(self.data[\"actions\"]), -1)\n",
    "\n",
    "        self.data[\"log_probs\"] = torch.cat(self.data[\"log_probs\"]).view(len(self.data[\"log_probs\"]), -1)\n",
    "\n",
    "        self.data[\"values\"] = torch.cat(self.data[\"values\"]).view(len(self.data[\"values\"]), -1)\n",
    "\n",
    "        self.data[\"rewards\"] = torch.tensor(np.stack(self.data[\"rewards\"]), dtype = torch.float32)\n",
    "        \n",
    "        self.data[\"dones\"] = torch.tensor(np.stack(self.data[\"dones\"]), dtype = torch.float32)\n",
    "    \n",
    "    def advantages_collector(self):\n",
    "\n",
    "        rewards = self.data[\"rewards\"]\n",
    "\n",
    "        values = self.data[\"values\"]\n",
    "\n",
    "        dones = self.data[\"dones\"]\n",
    "\n",
    "        T, n_envs = rewards.shape\n",
    "        \n",
    "        gae = torch.zeros(n_envs)\n",
    "        \n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            gae = delta + self.gamma * self.lambda_ * (1 - dones[t]) * gae\n",
    "            \n",
    "            advantages[t] = gae\n",
    "        \n",
    "        self.data[\"advantages\"] = advantages\n",
    "\n",
    "        self.data[\"values\"] = values[:-1]\n",
    "        \n",
    "        self.data[\"returns\"] = advantages + values[:-1]\n",
    "\n",
    "        returns = self.data[\"returns\"]\n",
    "\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        self.data[\"returns\"] = returns\n",
    "\n",
    "    def flatten(self):\n",
    "\n",
    "        self.data[\"observations\"] = torch.flatten(self.data[\"observations\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"actions\"] = torch.flatten(self.data[\"actions\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"log_probs\"] = torch.flatten(self.data[\"log_probs\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"values\"] = torch.flatten(self.data[\"values\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"returns\"] = torch.flatten(self.data[\"returns\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        advantages = torch.flatten(self.data[\"advantages\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"advantages\"] = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "\n",
    "    def training_loop(self):\n",
    "\n",
    "        dataset = Dataset(\n",
    "            self.data[\"observations\"],\n",
    "            self.data[\"actions\"],\n",
    "            self.data[\"log_probs\"],\n",
    "            self.data[\"values\"],\n",
    "            self.data[\"returns\"],\n",
    "            self.data[\"advantages\"]\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size = config['BATCH_SIZE'], shuffle = True)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = config['LEARNING_RATE'])\n",
    "        mse = nn.MSELoss()\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(config['N_EPOCHS']):\n",
    "        \n",
    "            clip_loss = 0.0\n",
    "            vf_loss = 0.0\n",
    "            entropy = 0.0\n",
    "            train_loss = 0.0\n",
    "        \n",
    "            for obs, acts, log_probs, values, returns, advantages in dataloader:\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                print(f'obs: {obs.shape} | {obs.dtype}')\n",
    "                \n",
    "                new_logits, new_values = self.model(obs)\n",
    "\n",
    "                new_distros = torch.distributions.Categorical(logits = new_logits)\n",
    "                \n",
    "                new_log_probs = new_distros.log_prob(acts.squeeze(-1))\n",
    "        \n",
    "                ratios = torch.exp(new_log_probs - log_probs)\n",
    "        \n",
    "                unclipped = ratios * advantages\n",
    "        \n",
    "                clipped = torch.clamp(ratios, 1 - config['EPSILON'], 1 + config['EPSILON']) * advantages\n",
    "        \n",
    "                L_CLIP = -torch.mean(torch.min(clipped, unclipped))\n",
    "        \n",
    "                L_VF = mse(new_values, returns)\n",
    "        \n",
    "                entropy = new_distros.entropy().mean()\n",
    "        \n",
    "                loss = L_CLIP + config['C1'] * L_VF - config['C2'] * entropy\n",
    "\n",
    "                clip_loss += L_CLIP.item()\n",
    "                vf_loss += L_VF.item()\n",
    "                entropy += entropy.item()\n",
    "                train_loss += loss.item()\n",
    "        \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), config['MAX_GRAD_NORM'])\n",
    "                optimizer.step()\n",
    "        \n",
    "            clip_loss /= len(dataloader)\n",
    "            vf_loss /= len(dataloader)\n",
    "            entropy /= len(dataloader)\n",
    "            train_loss /= len(dataloader)\n",
    "\n",
    "            print(f'Loss: {train_loss} | L_CLIP: {clip_loss} | L_VF: {vf_loss} | Entropy: {entropy}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64fed005-2811-4f6b-9d4e-75b2a103bce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ab244b37-7118-46d1-bb67-6ec87f4f20d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e35636d0-59b1-47d9-a44a-9509b395c4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4a855366-5adc-4755-a34f-0d1490f9ac3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "10090ea6-ea70-4265-b11f-9268fa55668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de55627-1c90-4e3b-8757-ebf6a54c86df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f08c4d-9749-4885-9201-1453b7e806f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f89ca6-c80c-49b3-81bc-ba78b72af621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d07da-792c-49b2-8622-3c71b4ab01ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5e6e3-5e3f-46fe-8910-481d6c2289d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
