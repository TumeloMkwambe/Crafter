{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7312626-916f-4327-a937-a4df977808fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluation_episode, image_display\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import stable_baselines3 as sb3\n",
    "from env_setup import init_env\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64fed005-2811-4f6b-9d4e-75b2a103bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, observations, actions, log_probs, values, returns, advantages):\n",
    "        \n",
    "        self.observations = observations\n",
    "        \n",
    "        self.actions = actions\n",
    "        \n",
    "        self.log_probs = log_probs\n",
    "        \n",
    "        self.values = values\n",
    "        \n",
    "        self.returns = returns\n",
    "        \n",
    "        self.advantages = advantages\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        obs = self.observations[idx]\n",
    "        \n",
    "        acts = self.actions[idx]\n",
    "        \n",
    "        log_probs = self.actions[idx]\n",
    "        \n",
    "        values = self.values[idx]\n",
    "        \n",
    "        returns = self.returns[idx]\n",
    "        \n",
    "        advantages = self.advantages[idx]\n",
    "\n",
    "        return obs, acts, log_probs, values, returns, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab244b37-7118-46d1-bb67-6ec87f4f20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "\n",
    "    def __init__(self, vec_env, model, n_steps, gamma, lambda_):\n",
    "        \n",
    "        self.vec_env = vec_env\n",
    "        self.model = model\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        self.data = {\"observations\": [], \"actions\": [], \"log_probs\": [], \"values\": [], \"rewards\": [], \"dones\": [], \"infos\": []}\n",
    "        self.count_steps = 0\n",
    "\n",
    "    def observation_preprocessing(self, observations):\n",
    "        \n",
    "        observations = torch.tensor(observations)\n",
    "        observations = observations.permute(0, 3, 1, 2) # from [N, H, W, C] to [N, C, H, W]\n",
    "        observations = observations.float() / 255.0\n",
    "        \n",
    "        return observations\n",
    "\n",
    "    def data_collection(self):\n",
    "        \n",
    "        observations = self.vec_env.reset()\n",
    "        \n",
    "        while self.count_steps < self.n_steps:\n",
    "\n",
    "            observations = self.observation_preprocessing(observations)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                logits, values = self.model(observations)\n",
    "                \n",
    "                distros = torch.distributions.Categorical(logits = logits)\n",
    "                \n",
    "                actions = distros.sample().unsqueeze(-1)\n",
    "                \n",
    "                log_probs = distros.log_prob(actions.squeeze(-1)).unsqueeze(-1)\n",
    "                \n",
    "            next_observations, rewards, dones, infos = self.vec_env.step(actions.cpu().numpy().squeeze(-1))\n",
    "            \n",
    "            self.data[\"observations\"].append(observations) \n",
    "            self.data[\"actions\"].append(actions)\n",
    "            self.data[\"log_probs\"].append(log_probs)\n",
    "            self.data[\"values\"].append(values)\n",
    "            self.data[\"rewards\"].append(rewards)\n",
    "            self.data[\"dones\"].append(dones)\n",
    "            self.data[\"infos\"].append(infos)\n",
    "            \n",
    "            self.count_steps += self.vec_env.num_envs\n",
    "            observations = next_observations\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            observations = self.observation_preprocessing(observations)\n",
    "\n",
    "            _, values = self.model(observations)\n",
    "\n",
    "            self.data[\"values\"].append(values)\n",
    "\n",
    "\n",
    "    def stack(self):\n",
    "\n",
    "        obs = self.data[\"observations\"][0].shape\n",
    "\n",
    "        self.data[\"observations\"] = torch.stack(self.data[\"observations\"])\n",
    "        \n",
    "        self.data[\"actions\"] = torch.cat(self.data[\"actions\"]).view(len(self.data[\"actions\"]), -1)\n",
    "\n",
    "        self.data[\"log_probs\"] = torch.cat(self.data[\"log_probs\"]).view(len(self.data[\"log_probs\"]), -1)\n",
    "\n",
    "        self.data[\"values\"] = torch.cat(self.data[\"values\"]).view(len(self.data[\"values\"]), -1)\n",
    "\n",
    "        self.data[\"rewards\"] = torch.tensor(np.stack(self.data[\"rewards\"]), dtype = torch.float32)\n",
    "        \n",
    "        self.data[\"dones\"] = torch.tensor(np.stack(self.data[\"dones\"]), dtype = torch.float32)\n",
    "    \n",
    "    def advantages_collector(self):\n",
    "\n",
    "        rewards = self.data[\"rewards\"]\n",
    "\n",
    "        values = self.data[\"values\"]\n",
    "\n",
    "        dones = self.data[\"dones\"]\n",
    "\n",
    "        T, n_envs = rewards.shape\n",
    "        \n",
    "        gae = torch.zeros(n_envs)\n",
    "        \n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            gae = delta + self.gamma * self.lambda_ * (1 - dones[t]) * gae\n",
    "            \n",
    "            advantages[t] = gae\n",
    "        \n",
    "        self.data[\"advantages\"] = advantages\n",
    "\n",
    "        self.data[\"values\"] = values[:-1]\n",
    "        \n",
    "        self.data[\"returns\"] = advantages + values[:-1]\n",
    "\n",
    "    def flatten(self):\n",
    "\n",
    "        self.data[\"observations\"] = torch.flatten(self.data[\"observations\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"actions\"] = torch.flatten(self.data[\"actions\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"log_probs\"] = torch.flatten(self.data[\"log_probs\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"values\"] = torch.flatten(self.data[\"values\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"returns\"] = torch.flatten(self.data[\"returns\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        advantages = torch.flatten(self.data[\"advantages\"], start_dim = 0, end_dim = 1)\n",
    "\n",
    "        self.data[\"advantages\"] = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "\n",
    "    def training_loop(self, n_epochs, batch_size):\n",
    "\n",
    "        dataset = Dataset(\n",
    "            self.data[\"observations\"],\n",
    "            self.data[\"actions\"],\n",
    "            self.data[\"log_probs\"],\n",
    "            self.data[\"values\"],\n",
    "            self.data[\"returns\"],\n",
    "            self.data[\"advantages\"]\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size = batch, shuffle = True)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            for obs, acts, log_probs, values, rewards, advantages in dataloader:\n",
    "                \n",
    "                new_logits, new_values = self.model(obs)\n",
    "                \n",
    "                new_distros = torch.distributions.Categorical(logits = logits)\n",
    "                \n",
    "                new_actions = new_distros.sample().unsqueeze(-1)\n",
    "                \n",
    "                new_log_probs = new_distros.log_prob(new_actions.squeeze(-1)).unsqueeze(-1)\n",
    "\n",
    "                ratio = torch.exp(new_log_probs - log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e35636d0-59b1-47d9-a44a-9509b395c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vec = sb3.common.env_util.make_vec_env(env_id = init_env, n_envs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b25b752a-3b7b-4fba-9854-d44083f18830",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = Actor_Critic(n_actions = 4)\n",
    "ppo = PPO(vec_env = env_vec, model = ac, n_steps = 15, gamma = 1, lambda_ = 3)\n",
    "ppo.data_collection()\n",
    "ppo.stack()\n",
    "ppo.advantages_collector()\n",
    "ppo.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93fa2050-6d6c-4fb3-a231-dd3e1820019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratios: tensor([0.2629, 0.0357, 0.0906, 0.2586, 0.0304], grad_fn=<ExpBackward0>)\n",
      "ratios: tensor([0.2473, 0.1162, 0.0809, 0.0377, 0.2464], grad_fn=<ExpBackward0>)\n",
      "ratios: tensor([0.0168, 0.2361, 0.0353, 0.0944, 0.0865], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 5\n",
    "\n",
    "dataset = Dataset(\n",
    "    ppo.data[\"observations\"],\n",
    "    ppo.data[\"actions\"],\n",
    "    ppo.data[\"log_probs\"],\n",
    "    ppo.data[\"values\"],\n",
    "    ppo.data[\"returns\"],\n",
    "    ppo.data[\"advantages\"]\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "ppo.model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for obs, acts, log_probs, values, returns, advantages in dataloader:\n",
    "        \n",
    "        new_logits, new_values = ac(obs)\n",
    "        \n",
    "        new_distros = torch.distributions.Categorical(logits = new_logits)\n",
    "        \n",
    "        new_actions = new_distros.sample()\n",
    "        \n",
    "        new_log_probs = new_distros.log_prob(new_actions)\n",
    "\n",
    "        ratios = torch.exp(new_log_probs - log_probs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642d771-a5e5-40fa-a0e8-5ffe9c7ec200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb9a9d-214b-449f-a880-3afccc2b3380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3379f-06f2-4b26-816d-065058449023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de55627-1c90-4e3b-8757-ebf6a54c86df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
